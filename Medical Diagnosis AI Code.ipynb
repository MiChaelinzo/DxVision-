{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0332c638-0ef6-4060-beaa-639c07a9b3b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting tensorflow\n  Using cached tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\nCollecting pytorch\n  Using cached pytorch-1.0.2.tar.gz (689 bytes)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nRequirement already satisfied: torchvision in /databricks/python3/lib/python3.10/site-packages (0.15.2+cpu)\nCollecting pydicom\n  Using cached pydicom-2.4.4-py3-none-any.whl (1.8 MB)\nRequirement already satisfied: transformers in /databricks/python3/lib/python3.10/site-packages (4.36.1)\nRequirement already satisfied: spacy in /databricks/python3/lib/python3.10/site-packages (3.7.2)\nCollecting lime\n  Using cached lime-0.2.0.1-py3-none-any.whl\nRequirement already satisfied: shap in /databricks/python3/lib/python3.10/site-packages (0.44.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (1.23.5)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (1.48.2)\nRequirement already satisfied: termcolor>=1.1.0 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (4.24.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: astunparse>=1.6.0 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: typing-extensions>=3.6.6 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (4.4.0)\nRequirement already satisfied: flatbuffers>=23.5.26 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: absl-py>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (1.0.0)\nCollecting h5py>=3.10.0\n  Using cached h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\nCollecting ml-dtypes~=0.3.1\n  Using cached ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\nRequirement already satisfied: requests<3,>=2.21.0 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (2.28.1)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (23.2)\nCollecting tensorboard<2.17,>=2.16\n  Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (0.35.0)\nRequirement already satisfied: wrapt>=1.11.0 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: libclang>=13.0.0 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (15.0.6.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nCollecting keras>=3.0.0\n  Using cached keras-3.3.3-py3-none-any.whl (1.1 MB)\nRequirement already satisfied: setuptools in /databricks/python3/lib/python3.10/site-packages (from tensorflow) (65.6.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /databricks/python3/lib/python3.10/site-packages (from torchvision) (9.4.0)\nRequirement already satisfied: torch==2.0.1 in /databricks/python3/lib/python3.10/site-packages (from torchvision) (2.0.1+cpu)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (3.1.2)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (2.8.4)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (3.9.0)\nRequirement already satisfied: sympy in /databricks/python3/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (1.11.1)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /databricks/python3/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.10/site-packages (from transformers) (2022.7.9)\nRequirement already satisfied: safetensors>=0.3.1 in /databricks/python3/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /databricks/python3/lib/python3.10/site-packages (from transformers) (0.19.4)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /databricks/python3/lib/python3.10/site-packages (from spacy) (3.0.12)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /databricks/python3/lib/python3.10/site-packages (from spacy) (2.4.8)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /databricks/python3/lib/python3.10/site-packages (from spacy) (1.1.2)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /databricks/python3/lib/python3.10/site-packages (from spacy) (0.9.0)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /databricks/python3/lib/python3.10/site-packages (from spacy) (3.0.9)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /databricks/python3/lib/python3.10/site-packages (from spacy) (1.0.10)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /databricks/python3/lib/python3.10/site-packages (from spacy) (1.10.6)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /databricks/python3/lib/python3.10/site-packages (from spacy) (5.2.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /databricks/python3/lib/python3.10/site-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /databricks/python3/lib/python3.10/site-packages (from spacy) (0.3.4)\nRequirement already satisfied: thinc<8.3.0,>=8.1.8 in /databricks/python3/lib/python3.10/site-packages (from spacy) (8.2.2)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from spacy) (2.0.8)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /databricks/python3/lib/python3.10/site-packages (from spacy) (3.3.0)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from spacy) (1.0.5)\nRequirement already satisfied: scikit-learn>=0.18 in /databricks/python3/lib/python3.10/site-packages (from lime) (1.1.1)\nCollecting scikit-image>=0.12\n  Using cached scikit_image-0.23.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\nRequirement already satisfied: matplotlib in /databricks/python3/lib/python3.10/site-packages (from lime) (3.7.0)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.10/site-packages (from lime) (1.10.0)\nRequirement already satisfied: slicer==0.0.7 in /databricks/python3/lib/python3.10/site-packages (from shap) (0.0.7)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.10/site-packages (from shap) (1.5.3)\nRequirement already satisfied: cloudpickle in /databricks/python3/lib/python3.10/site-packages (from shap) (2.0.0)\nRequirement already satisfied: numba in /databricks/python3/lib/python3.10/site-packages (from shap) (0.56.4)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /databricks/python3/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\nCollecting namex\n  Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\nCollecting rich\n  Using cached rich-13.7.1-py3-none-any.whl (240 kB)\nCollecting optree\n  Using cached optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2022.12.7)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\nCollecting lazy-loader>=0.4\n  Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\nCollecting tifffile>=2022.8.12\n  Using cached tifffile-2024.4.24-py3-none-any.whl (225 kB)\nCollecting imageio>=2.33\n  Using cached imageio-2.34.1-py3-none-any.whl (313 kB)\nRequirement already satisfied: joblib>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn>=0.18->lime) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn>=0.18->lime) (2.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /databricks/python3/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.4.1)\nCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n  Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /databricks/python3/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (2.2.2)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /databricks/python3/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /databricks/python3/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /databricks/python3/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /databricks/python3/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.10/site-packages (from jinja2->torch==2.0.1->torchvision) (2.1.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib->lime) (1.4.4)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.10/site-packages (from matplotlib->lime) (0.11.0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib->lime) (1.0.5)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.10/site-packages (from matplotlib->lime) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib->lime) (4.25.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib->lime) (3.0.9)\nRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /databricks/python3/lib/python3.10/site-packages (from numba->shap) (0.39.1)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas->shap) (2022.7)\nCollecting pygments<3.0.0,>=2.13.0\n  Using cached pygments-2.17.2-py3-none-any.whl (1.2 MB)\nCollecting markdown-it-py>=2.2.0\n  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\nRequirement already satisfied: mpmath>=0.19 in /databricks/python3/lib/python3.10/site-packages (from sympy->torch==2.0.1->torchvision) (1.2.1)\nCollecting mdurl~=0.1\n  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nBuilding wheels for collected packages: pytorch\n  Building wheel for pytorch (setup.py): started\n  Building wheel for pytorch (setup.py): finished with status 'error'\n  error: subprocess-exited-with-error\n  \n  × python setup.py bdist_wheel did not run successfully.\n  │ exit code: 1\n  ╰─> [6 lines of output]\n      Traceback (most recent call last):\n        File \"<string>\", line 2, in <module>\n        File \"<pip-setuptools-caller>\", line 34, in <module>\n        File \"/tmp/pip-install-zykz0zo3/pytorch_051fdd79321541179f801818a1418cfc/setup.py\", line 15, in <module>\n          raise Exception(message)\n      Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for pytorch\n  Running setup.py clean for pytorch\nFailed to build pytorch\nInstalling collected packages: pytorch, namex, tifffile, pygments, pydicom, protobuf, optree, ml-dtypes, mdurl, lazy-loader, imageio, h5py, tensorboard, scikit-image, markdown-it-py, rich, lime, keras, tensorflow\n  Running setup.py install for pytorch: started\n  Running setup.py install for pytorch: finished with status 'error'\n  error: subprocess-exited-with-error\n  \n  × Running setup.py install for pytorch did not run successfully.\n  │ exit code: 1\n  ╰─> [6 lines of output]\n      Traceback (most recent call last):\n        File \"<string>\", line 2, in <module>\n        File \"<pip-setuptools-caller>\", line 34, in <module>\n        File \"/tmp/pip-install-zykz0zo3/pytorch_051fdd79321541179f801818a1418cfc/setup.py\", line 11, in <module>\n          raise Exception(message)\n      Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: legacy-install-failure\n\n× Encountered error while trying to install package.\n╰─> pytorch\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for output from the failure.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mCalledProcessError\u001B[0m                        Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2564373843470362>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Install necessary libraries\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_line_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpip\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minstall tensorflow pytorch torchvision pydicom transformers spacy lime shap\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2417\u001B[0m, in \u001B[0;36mInteractiveShell.run_line_magic\u001B[0;34m(self, magic_name, line, _stack_depth)\u001B[0m\n",
       "\u001B[1;32m   2415\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal_ns\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_local_scope(stack_depth)\n",
       "\u001B[1;32m   2416\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n",
       "\u001B[0;32m-> 2417\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2419\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n",
       "\u001B[1;32m   2420\u001B[0m \u001B[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001B[39;00m\n",
       "\u001B[1;32m   2421\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n",
       "\u001B[1;32m   2422\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/PipMagicOverrides.py:33\u001B[0m, in \u001B[0;36mPipMagicOverrides.pip\u001B[0;34m(self, line)\u001B[0m\n",
       "\u001B[1;32m     31\u001B[0m \u001B[38;5;129m@line_magic\u001B[39m\n",
       "\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpip\u001B[39m(\u001B[38;5;28mself\u001B[39m, line):\n",
       "\u001B[0;32m---> 33\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpipMagicHandler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunCmd\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpip\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mline\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/PipMagicOverrides.py:59\u001B[0m, in \u001B[0;36mPipMagicHandler.runCmd\u001B[0;34m(self, magicCmd, line)\u001B[0m\n",
       "\u001B[1;32m     57\u001B[0m     \u001B[38;5;28mprint\u001B[39m(PYTHON_RESTART_WARNING)\n",
       "\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m parsedResult\u001B[38;5;241m.\u001B[39mrewrittenCommand():\n",
       "\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecutePipCommand\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparsedResult\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     60\u001B[0m envManager\u001B[38;5;241m.\u001B[39mpostExecute(parsedResult)\n",
       "\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m parsedResult\u001B[38;5;241m.\u001B[39misMutation():\n",
       "\u001B[1;32m     62\u001B[0m     \u001B[38;5;66;03m# double print this output is at the end so it is more\u001B[39;00m\n",
       "\u001B[1;32m     63\u001B[0m     \u001B[38;5;66;03m# likely to be seen\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/PipMagicOverrides.py:118\u001B[0m, in \u001B[0;36mPipMagicHandler.executePipCommand\u001B[0;34m(self, result)\u001B[0m\n",
       "\u001B[1;32m    116\u001B[0m     sys\u001B[38;5;241m.\u001B[39mstdout\u001B[38;5;241m.\u001B[39mflush()\n",
       "\u001B[1;32m    117\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m returncode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\u001B[0;32m--> 118\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m subprocess\u001B[38;5;241m.\u001B[39mCalledProcessError(returncode, origCmd)\n",
       "\u001B[1;32m    119\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m    120\u001B[0m     end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
       "\n",
       "\u001B[0;31mCalledProcessError\u001B[0m: Command 'pip --disable-pip-version-check install tensorflow pytorch torchvision pydicom transformers spacy lime shap' returned non-zero exit status 1."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "CalledProcessError",
        "evalue": "Command 'pip --disable-pip-version-check install tensorflow pytorch torchvision pydicom transformers spacy lime shap' returned non-zero exit status 1."
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>CalledProcessError</span>: Command 'pip --disable-pip-version-check install tensorflow pytorch torchvision pydicom transformers spacy lime shap' returned non-zero exit status 1."
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mCalledProcessError\u001B[0m                        Traceback (most recent call last)",
        "File \u001B[0;32m<command-2564373843470362>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Install necessary libraries\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_line_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpip\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minstall tensorflow pytorch torchvision pydicom transformers spacy lime shap\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2417\u001B[0m, in \u001B[0;36mInteractiveShell.run_line_magic\u001B[0;34m(self, magic_name, line, _stack_depth)\u001B[0m\n\u001B[1;32m   2415\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal_ns\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_local_scope(stack_depth)\n\u001B[1;32m   2416\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[0;32m-> 2417\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2419\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[1;32m   2420\u001B[0m \u001B[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001B[39;00m\n\u001B[1;32m   2421\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[1;32m   2422\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
        "File \u001B[0;32m/databricks/python_shell/dbruntime/PipMagicOverrides.py:33\u001B[0m, in \u001B[0;36mPipMagicOverrides.pip\u001B[0;34m(self, line)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;129m@line_magic\u001B[39m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpip\u001B[39m(\u001B[38;5;28mself\u001B[39m, line):\n\u001B[0;32m---> 33\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpipMagicHandler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunCmd\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpip\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mline\u001B[49m\u001B[43m)\u001B[49m\n",
        "File \u001B[0;32m/databricks/python_shell/dbruntime/PipMagicOverrides.py:59\u001B[0m, in \u001B[0;36mPipMagicHandler.runCmd\u001B[0;34m(self, magicCmd, line)\u001B[0m\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28mprint\u001B[39m(PYTHON_RESTART_WARNING)\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m parsedResult\u001B[38;5;241m.\u001B[39mrewrittenCommand():\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecutePipCommand\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparsedResult\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m envManager\u001B[38;5;241m.\u001B[39mpostExecute(parsedResult)\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m parsedResult\u001B[38;5;241m.\u001B[39misMutation():\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;66;03m# double print this output is at the end so it is more\u001B[39;00m\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;66;03m# likely to be seen\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python_shell/dbruntime/PipMagicOverrides.py:118\u001B[0m, in \u001B[0;36mPipMagicHandler.executePipCommand\u001B[0;34m(self, result)\u001B[0m\n\u001B[1;32m    116\u001B[0m     sys\u001B[38;5;241m.\u001B[39mstdout\u001B[38;5;241m.\u001B[39mflush()\n\u001B[1;32m    117\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m returncode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 118\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m subprocess\u001B[38;5;241m.\u001B[39mCalledProcessError(returncode, origCmd)\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    120\u001B[0m     end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
        "\u001B[0;31mCalledProcessError\u001B[0m: Command 'pip --disable-pip-version-check install tensorflow pytorch torchvision pydicom transformers spacy lime shap' returned non-zero exit status 1."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "%pip install tensorflow pytorch torchvision pydicom transformers spacy lime shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "212091f6-c8d7-4af7-9ad3-61ed39325339",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.10/site-packages (1.5.3)\nRequirement already satisfied: pillow in /databricks/python3/lib/python3.10/site-packages (9.4.0)\nRequirement already satisfied: pydicom in /local_disk0/.ephemeral_nfs/envs/pythonEnv-18021307-8031-4ab3-bd8f-531555053374/lib/python3.10/site-packages (2.4.4)\nRequirement already satisfied: numpy>=1.21.0 in /databricks/python3/lib/python3.10/site-packages (from pandas) (1.23.5)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas) (2022.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.10/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas pillow pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd34fbab-a07c-494a-b972-a4a20ae9582d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3028, 3030)                                     UN: Array of 1251 elements\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-18021307-8031-4ab3-bd8f-531555053374/lib/python3.10/site-packages/pydicom/dataelem.py:847: UserWarning: Unknown DICOM tag (3028, 3030) - setting VR to 'UN'\n  warnings.warn(msg + \" - setting VR to 'UN'\")\n"
     ]
    }
   ],
   "source": [
    "print(dicom_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d052d81-4998-4aed-a115-cae03945acc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mInvalidDicomError\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2564373843470363>, line 10\u001B[0m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Load medical image (example with DICOM)\u001B[39;00m\n",
       "\u001B[1;32m      9\u001B[0m image_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmedical_image.dcm\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m---> 10\u001B[0m dicom_data \u001B[38;5;241m=\u001B[39m pydicom\u001B[38;5;241m.\u001B[39mdcmread(image_path)\n",
       "\u001B[1;32m     11\u001B[0m image_array \u001B[38;5;241m=\u001B[39m dicom_data\u001B[38;5;241m.\u001B[39mpixel_array\n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Preprocess image (resize, normalization)\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-18021307-8031-4ab3-bd8f-531555053374/lib/python3.10/site-packages/pydicom/filereader.py:1030\u001B[0m, in \u001B[0;36mdcmread\u001B[0;34m(fp, defer_size, stop_before_pixels, force, specific_tags)\u001B[0m\n",
       "\u001B[1;32m   1028\u001B[0m     stop_when \u001B[38;5;241m=\u001B[39m _at_pixel_data\n",
       "\u001B[1;32m   1029\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m-> 1030\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[43mread_partial\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1031\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfp\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1032\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstop_when\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1033\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdefer_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize_in_bytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdefer_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1034\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1035\u001B[0m \u001B[43m        \u001B[49m\u001B[43mspecific_tags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspecific_tags\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1036\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1038\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m caller_owns_file:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-18021307-8031-4ab3-bd8f-531555053374/lib/python3.10/site-packages/pydicom/filereader.py:778\u001B[0m, in \u001B[0;36mread_partial\u001B[0;34m(fileobj, stop_when, defer_size, force, specific_tags)\u001B[0m\n",
       "\u001B[1;32m    745\u001B[0m \u001B[38;5;124;03m\"\"\"Parse a DICOM file until a condition is met.\u001B[39;00m\n",
       "\u001B[1;32m    746\u001B[0m \n",
       "\u001B[1;32m    747\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    773\u001B[0m \u001B[38;5;124;03m    More generic file reading function.\u001B[39;00m\n",
       "\u001B[1;32m    774\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    775\u001B[0m \u001B[38;5;66;03m# Read File Meta Information\u001B[39;00m\n",
       "\u001B[1;32m    776\u001B[0m \n",
       "\u001B[1;32m    777\u001B[0m \u001B[38;5;66;03m# Read preamble (if present)\u001B[39;00m\n",
       "\u001B[0;32m--> 778\u001B[0m preamble \u001B[38;5;241m=\u001B[39m \u001B[43mread_preamble\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfileobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforce\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    779\u001B[0m \u001B[38;5;66;03m# Read any File Meta Information group (0002,eeee) elements (if present)\u001B[39;00m\n",
       "\u001B[1;32m    780\u001B[0m file_meta \u001B[38;5;241m=\u001B[39m _read_file_meta_info(fileobj)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-18021307-8031-4ab3-bd8f-531555053374/lib/python3.10/site-packages/pydicom/filereader.py:723\u001B[0m, in \u001B[0;36mread_preamble\u001B[0;34m(fp, force)\u001B[0m\n",
       "\u001B[1;32m    720\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    722\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m magic \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDICM\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m force:\n",
       "\u001B[0;32m--> 723\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m InvalidDicomError(\n",
       "\u001B[1;32m    724\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFile is missing DICOM File Meta Information header or the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDICM\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    725\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprefix is missing from the header. Use force=True to force \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    726\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreading.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    727\u001B[0m     )\n",
       "\u001B[1;32m    728\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    729\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfp\u001B[38;5;241m.\u001B[39mtell() \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m4\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m08x\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDICM\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m prefix found\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mInvalidDicomError\u001B[0m: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "InvalidDicomError",
        "evalue": "File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading."
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>InvalidDicomError</span>: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading."
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mInvalidDicomError\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-2564373843470363>, line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Load medical image (example with DICOM)\u001B[39;00m\n\u001B[1;32m      9\u001B[0m image_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmedical_image.dcm\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 10\u001B[0m dicom_data \u001B[38;5;241m=\u001B[39m pydicom\u001B[38;5;241m.\u001B[39mdcmread(image_path)\n\u001B[1;32m     11\u001B[0m image_array \u001B[38;5;241m=\u001B[39m dicom_data\u001B[38;5;241m.\u001B[39mpixel_array\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Preprocess image (resize, normalization)\u001B[39;00m\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-18021307-8031-4ab3-bd8f-531555053374/lib/python3.10/site-packages/pydicom/filereader.py:1030\u001B[0m, in \u001B[0;36mdcmread\u001B[0;34m(fp, defer_size, stop_before_pixels, force, specific_tags)\u001B[0m\n\u001B[1;32m   1028\u001B[0m     stop_when \u001B[38;5;241m=\u001B[39m _at_pixel_data\n\u001B[1;32m   1029\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1030\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[43mread_partial\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1031\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1032\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstop_when\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1033\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdefer_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize_in_bytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdefer_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1034\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1035\u001B[0m \u001B[43m        \u001B[49m\u001B[43mspecific_tags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspecific_tags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1036\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1038\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m caller_owns_file:\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-18021307-8031-4ab3-bd8f-531555053374/lib/python3.10/site-packages/pydicom/filereader.py:778\u001B[0m, in \u001B[0;36mread_partial\u001B[0;34m(fileobj, stop_when, defer_size, force, specific_tags)\u001B[0m\n\u001B[1;32m    745\u001B[0m \u001B[38;5;124;03m\"\"\"Parse a DICOM file until a condition is met.\u001B[39;00m\n\u001B[1;32m    746\u001B[0m \n\u001B[1;32m    747\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    773\u001B[0m \u001B[38;5;124;03m    More generic file reading function.\u001B[39;00m\n\u001B[1;32m    774\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    775\u001B[0m \u001B[38;5;66;03m# Read File Meta Information\u001B[39;00m\n\u001B[1;32m    776\u001B[0m \n\u001B[1;32m    777\u001B[0m \u001B[38;5;66;03m# Read preamble (if present)\u001B[39;00m\n\u001B[0;32m--> 778\u001B[0m preamble \u001B[38;5;241m=\u001B[39m \u001B[43mread_preamble\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfileobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforce\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    779\u001B[0m \u001B[38;5;66;03m# Read any File Meta Information group (0002,eeee) elements (if present)\u001B[39;00m\n\u001B[1;32m    780\u001B[0m file_meta \u001B[38;5;241m=\u001B[39m _read_file_meta_info(fileobj)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-18021307-8031-4ab3-bd8f-531555053374/lib/python3.10/site-packages/pydicom/filereader.py:723\u001B[0m, in \u001B[0;36mread_preamble\u001B[0;34m(fp, force)\u001B[0m\n\u001B[1;32m    720\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    722\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m magic \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDICM\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m force:\n\u001B[0;32m--> 723\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m InvalidDicomError(\n\u001B[1;32m    724\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFile is missing DICOM File Meta Information header or the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDICM\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    725\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprefix is missing from the header. Use force=True to force \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    726\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreading.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    727\u001B[0m     )\n\u001B[1;32m    728\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    729\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfp\u001B[38;5;241m.\u001B[39mtell() \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m4\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m08x\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDICM\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m prefix found\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "\u001B[0;31mInvalidDicomError\u001B[0m: File is missing DICOM File Meta Information header or the 'DICM' prefix is missing from the header. Use force=True to force reading."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pydicom\n",
    "\n",
    "# Load patient data (assuming CSV format)\n",
    "patient_data = pd.read_csv(\"patient_data.csv\")\n",
    "\n",
    "# Load medical image (example with DICOM)\n",
    "image_path = \"medical_image.dcm\"\n",
    "dicom_data = pydicom.dcmread(image_path)\n",
    "image_array = dicom_data.pixel_array\n",
    "\n",
    "# Preprocess image (resize, normalization)\n",
    "image = Image.fromarray(image_array)\n",
    "image = image.resize((224, 224))  # Assuming model input size\n",
    "image_array = np.array(image) / 255.0  # Normalize pixel values\n",
    "\n",
    "# ... further preprocessing based on data and model requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d13410a-9722-438c-983c-dd7c993701f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-151638418117555>, line 11\u001B[0m\n",
       "\u001B[1;32m      9\u001B[0m image_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmedical_image.dcm\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     10\u001B[0m dicom_data \u001B[38;5;241m=\u001B[39m pydicom\u001B[38;5;241m.\u001B[39mdcmread(image_path, force\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[0;32m---> 11\u001B[0m image_array \u001B[38;5;241m=\u001B[39m dicom_data\u001B[38;5;241m.\u001B[39mpixel_array\n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Preprocess image (resize, normalization)\u001B[39;00m\n",
       "\u001B[1;32m     14\u001B[0m image \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(image_array)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-18021307-8031-4ab3-bd8f-531555053374/lib/python3.10/site-packages/pydicom/dataset.py:908\u001B[0m, in \u001B[0;36mDataset.__getattr__\u001B[0;34m(self, name)\u001B[0m\n",
       "\u001B[1;32m    906\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {}\n",
       "\u001B[1;32m    907\u001B[0m \u001B[38;5;66;03m# Try the base class attribute getter (fix for issue 332)\u001B[39;00m\n",
       "\u001B[0;32m--> 908\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mobject\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getattribute__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-18021307-8031-4ab3-bd8f-531555053374/lib/python3.10/site-packages/pydicom/dataset.py:1955\u001B[0m, in \u001B[0;36mDataset.pixel_array\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1940\u001B[0m \u001B[38;5;129m@property\u001B[39m\n",
       "\u001B[1;32m   1941\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpixel_array\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy.ndarray\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m   1942\u001B[0m     \u001B[38;5;124;03m\"\"\"Return the pixel data as a :class:`numpy.ndarray`.\u001B[39;00m\n",
       "\u001B[1;32m   1943\u001B[0m \n",
       "\u001B[1;32m   1944\u001B[0m \u001B[38;5;124;03m    .. versionchanged:: 1.4\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1953\u001B[0m \u001B[38;5;124;03m        :class:`numpy.ndarray`.\u001B[39;00m\n",
       "\u001B[1;32m   1954\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 1955\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_pixel_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1956\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy.ndarray\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pixel_array)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-18021307-8031-4ab3-bd8f-531555053374/lib/python3.10/site-packages/pydicom/dataset.py:1512\u001B[0m, in \u001B[0;36mDataset.convert_pixel_data\u001B[0;34m(self, handler_name)\u001B[0m\n",
       "\u001B[1;32m   1510\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_pixel_data_using_handler(handler_name)\n",
       "\u001B[1;32m   1511\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1512\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_pixel_data_without_handler\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-18021307-8031-4ab3-bd8f-531555053374/lib/python3.10/site-packages/pydicom/dataset.py:1555\u001B[0m, in \u001B[0;36mDataset._convert_pixel_data_without_handler\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1551\u001B[0m \u001B[38;5;124;03m\"\"\"Convert the pixel data using the first matching handler.\u001B[39;00m\n",
       "\u001B[1;32m   1552\u001B[0m \u001B[38;5;124;03mSee :meth:`~Dataset.convert_pixel_data` for more information.\u001B[39;00m\n",
       "\u001B[1;32m   1553\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1554\u001B[0m \u001B[38;5;66;03m# Find all possible handlers that support the transfer syntax\u001B[39;00m\n",
       "\u001B[0;32m-> 1555\u001B[0m ts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfile_meta\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTransferSyntaxUID\u001B[49m\n",
       "\u001B[1;32m   1556\u001B[0m possible_handlers \u001B[38;5;241m=\u001B[39m [\n",
       "\u001B[1;32m   1557\u001B[0m     hh \u001B[38;5;28;01mfor\u001B[39;00m hh \u001B[38;5;129;01min\u001B[39;00m pydicom\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpixel_data_handlers\n",
       "\u001B[1;32m   1558\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m hh \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   1559\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m hh\u001B[38;5;241m.\u001B[39msupports_transfer_syntax(ts)\n",
       "\u001B[1;32m   1560\u001B[0m ]\n",
       "\u001B[1;32m   1562\u001B[0m \u001B[38;5;66;03m# No handlers support the transfer syntax\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-18021307-8031-4ab3-bd8f-531555053374/lib/python3.10/site-packages/pydicom/dataset.py:908\u001B[0m, in \u001B[0;36mDataset.__getattr__\u001B[0;34m(self, name)\u001B[0m\n",
       "\u001B[1;32m    906\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {}\n",
       "\u001B[1;32m    907\u001B[0m \u001B[38;5;66;03m# Try the base class attribute getter (fix for issue 332)\u001B[39;00m\n",
       "\u001B[0;32m--> 908\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mobject\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getattribute__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'FileMetaDataset' object has no attribute 'TransferSyntaxUID'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AttributeError",
        "evalue": "'FileMetaDataset' object has no attribute 'TransferSyntaxUID'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'FileMetaDataset' object has no attribute 'TransferSyntaxUID'"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
        "File \u001B[0;32m<command-151638418117555>, line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m image_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmedical_image.dcm\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     10\u001B[0m dicom_data \u001B[38;5;241m=\u001B[39m pydicom\u001B[38;5;241m.\u001B[39mdcmread(image_path, force\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 11\u001B[0m image_array \u001B[38;5;241m=\u001B[39m dicom_data\u001B[38;5;241m.\u001B[39mpixel_array\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Preprocess image (resize, normalization)\u001B[39;00m\n\u001B[1;32m     14\u001B[0m image \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(image_array)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-18021307-8031-4ab3-bd8f-531555053374/lib/python3.10/site-packages/pydicom/dataset.py:908\u001B[0m, in \u001B[0;36mDataset.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    906\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {}\n\u001B[1;32m    907\u001B[0m \u001B[38;5;66;03m# Try the base class attribute getter (fix for issue 332)\u001B[39;00m\n\u001B[0;32m--> 908\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mobject\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getattribute__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-18021307-8031-4ab3-bd8f-531555053374/lib/python3.10/site-packages/pydicom/dataset.py:1955\u001B[0m, in \u001B[0;36mDataset.pixel_array\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1940\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m   1941\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpixel_array\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy.ndarray\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1942\u001B[0m     \u001B[38;5;124;03m\"\"\"Return the pixel data as a :class:`numpy.ndarray`.\u001B[39;00m\n\u001B[1;32m   1943\u001B[0m \n\u001B[1;32m   1944\u001B[0m \u001B[38;5;124;03m    .. versionchanged:: 1.4\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1953\u001B[0m \u001B[38;5;124;03m        :class:`numpy.ndarray`.\u001B[39;00m\n\u001B[1;32m   1954\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1955\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_pixel_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1956\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy.ndarray\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pixel_array)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-18021307-8031-4ab3-bd8f-531555053374/lib/python3.10/site-packages/pydicom/dataset.py:1512\u001B[0m, in \u001B[0;36mDataset.convert_pixel_data\u001B[0;34m(self, handler_name)\u001B[0m\n\u001B[1;32m   1510\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_pixel_data_using_handler(handler_name)\n\u001B[1;32m   1511\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1512\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_pixel_data_without_handler\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-18021307-8031-4ab3-bd8f-531555053374/lib/python3.10/site-packages/pydicom/dataset.py:1555\u001B[0m, in \u001B[0;36mDataset._convert_pixel_data_without_handler\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1551\u001B[0m \u001B[38;5;124;03m\"\"\"Convert the pixel data using the first matching handler.\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;124;03mSee :meth:`~Dataset.convert_pixel_data` for more information.\u001B[39;00m\n\u001B[1;32m   1553\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1554\u001B[0m \u001B[38;5;66;03m# Find all possible handlers that support the transfer syntax\u001B[39;00m\n\u001B[0;32m-> 1555\u001B[0m ts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfile_meta\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTransferSyntaxUID\u001B[49m\n\u001B[1;32m   1556\u001B[0m possible_handlers \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m   1557\u001B[0m     hh \u001B[38;5;28;01mfor\u001B[39;00m hh \u001B[38;5;129;01min\u001B[39;00m pydicom\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpixel_data_handlers\n\u001B[1;32m   1558\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m hh \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m hh\u001B[38;5;241m.\u001B[39msupports_transfer_syntax(ts)\n\u001B[1;32m   1560\u001B[0m ]\n\u001B[1;32m   1562\u001B[0m \u001B[38;5;66;03m# No handlers support the transfer syntax\u001B[39;00m\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-18021307-8031-4ab3-bd8f-531555053374/lib/python3.10/site-packages/pydicom/dataset.py:908\u001B[0m, in \u001B[0;36mDataset.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    906\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {}\n\u001B[1;32m    907\u001B[0m \u001B[38;5;66;03m# Try the base class attribute getter (fix for issue 332)\u001B[39;00m\n\u001B[0;32m--> 908\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mobject\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getattribute__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
        "\u001B[0;31mAttributeError\u001B[0m: 'FileMetaDataset' object has no attribute 'TransferSyntaxUID'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pydicom\n",
    "\n",
    "# Load patient data (assuming CSV format)\n",
    "patient_data = pd.read_csv(\"patient_data.csv\")\n",
    "\n",
    "# Load medical image (example with DICOM)\n",
    "image_path = \"medical_image.dcm\"\n",
    "dicom_data = pydicom.dcmread(image_path, force=True)\n",
    "image_array = dicom_data.pixel_array\n",
    "\n",
    "# Preprocess image (resize, normalization)\n",
    "image = Image.fromarray(image_array)\n",
    "image = image.resize((224, 224))  # Assuming model input size\n",
    "image_array = np.array(image) / 255.0  # Normalize pixel values\n",
    "\n",
    "# ... further preprocessing based on data and model requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d70fa706-5d62-4ee3-87b6-a04e31549a05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2564373843470364>, line 19\u001B[0m\n",
       "\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Compile and train the model\u001B[39;00m\n",
       "\u001B[1;32m     18\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary_crossentropy\u001B[39m\u001B[38;5;124m\"\u001B[39m, optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m\"\u001B[39m, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
       "\u001B[0;32m---> 19\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(image_data, labels, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n",
       "\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Example: RNN for text classification\u001B[39;00m\n",
       "\u001B[1;32m     22\u001B[0m model_rnn \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mSequential([\n",
       "\u001B[1;32m     23\u001B[0m     layers\u001B[38;5;241m.\u001B[39mEmbedding(vocab_size, embedding_dim, input_length\u001B[38;5;241m=\u001B[39mmax_length),\n",
       "\u001B[1;32m     24\u001B[0m     layers\u001B[38;5;241m.\u001B[39mBidirectional(layers\u001B[38;5;241m.\u001B[39mGRU(\u001B[38;5;241m32\u001B[39m)),\n",
       "\u001B[1;32m     25\u001B[0m     layers\u001B[38;5;241m.\u001B[39mDense(\u001B[38;5;241m1\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msigmoid\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m     26\u001B[0m ])\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
       "\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n",
       "\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n",
       "\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1105\u001B[0m, in \u001B[0;36mselect_data_adapter\u001B[0;34m(x, y)\u001B[0m\n",
       "\u001B[1;32m   1102\u001B[0m adapter_cls \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mcls\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01min\u001B[39;00m ALL_ADAPTER_CLS \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mcan_handle(x, y)]\n",
       "\u001B[1;32m   1103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m adapter_cls:\n",
       "\u001B[1;32m   1104\u001B[0m     \u001B[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001B[39;00m\n",
       "\u001B[0;32m-> 1105\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m   1106\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to find data adapter that can handle input: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n",
       "\u001B[1;32m   1107\u001B[0m             _type_name(x), _type_name(y)\n",
       "\u001B[1;32m   1108\u001B[0m         )\n",
       "\u001B[1;32m   1109\u001B[0m     )\n",
       "\u001B[1;32m   1110\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(adapter_cls) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
       "\u001B[1;32m   1111\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n",
       "\u001B[1;32m   1112\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData adapters should be mutually exclusive for \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1113\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhandling inputs. Found multiple adapters \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m to handle \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1114\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(adapter_cls, _type_name(x), _type_name(y))\n",
       "\u001B[1;32m   1115\u001B[0m     )\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: Failed to find data adapter that can handle input: <class 'str'>, <class 'ellipsis'>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ValueError",
        "evalue": "Failed to find data adapter that can handle input: <class 'str'>, <class 'ellipsis'>"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: Failed to find data adapter that can handle input: <class 'str'>, <class 'ellipsis'>"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
        "File \u001B[0;32m<command-2564373843470364>, line 19\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Compile and train the model\u001B[39;00m\n\u001B[1;32m     18\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary_crossentropy\u001B[39m\u001B[38;5;124m\"\u001B[39m, optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m\"\u001B[39m, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m---> 19\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(image_data, labels, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Example: RNN for text classification\u001B[39;00m\n\u001B[1;32m     22\u001B[0m model_rnn \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mSequential([\n\u001B[1;32m     23\u001B[0m     layers\u001B[38;5;241m.\u001B[39mEmbedding(vocab_size, embedding_dim, input_length\u001B[38;5;241m=\u001B[39mmax_length),\n\u001B[1;32m     24\u001B[0m     layers\u001B[38;5;241m.\u001B[39mBidirectional(layers\u001B[38;5;241m.\u001B[39mGRU(\u001B[38;5;241m32\u001B[39m)),\n\u001B[1;32m     25\u001B[0m     layers\u001B[38;5;241m.\u001B[39mDense(\u001B[38;5;241m1\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msigmoid\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     26\u001B[0m ])\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1105\u001B[0m, in \u001B[0;36mselect_data_adapter\u001B[0;34m(x, y)\u001B[0m\n\u001B[1;32m   1102\u001B[0m adapter_cls \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mcls\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01min\u001B[39;00m ALL_ADAPTER_CLS \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mcan_handle(x, y)]\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m adapter_cls:\n\u001B[1;32m   1104\u001B[0m     \u001B[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001B[39;00m\n\u001B[0;32m-> 1105\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1106\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to find data adapter that can handle input: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1107\u001B[0m             _type_name(x), _type_name(y)\n\u001B[1;32m   1108\u001B[0m         )\n\u001B[1;32m   1109\u001B[0m     )\n\u001B[1;32m   1110\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(adapter_cls) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   1111\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1112\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData adapters should be mutually exclusive for \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1113\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhandling inputs. Found multiple adapters \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m to handle \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1114\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001B[1;32m   1115\u001B[0m     )\n",
        "\u001B[0;31mValueError\u001B[0m: Failed to find data adapter that can handle input: <class 'str'>, <class 'ellipsis'>"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the image data and labels (replace with your actual data)\n",
    "image_data = (\"medical_image.dcm\")\n",
    "labels = ...\n",
    "image_file_paths = ['medical_image1.dcm', 'medical_image2.dcm', ...]  # List of image file paths\n",
    "\n",
    "# Define a CNN model for image analysis (example)\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(224, 224, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    # ... additional layers\n",
    "    layers.Dense(1, activation=\"sigmoid\")  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(image_data, labels, epochs=10)\n",
    "\n",
    "# Example: RNN for text classification\n",
    "model_rnn = tf.keras.Sequential([\n",
    "    layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    layers.Bidirectional(layers.GRU(32)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Example: Transformers for text classification\n",
    "model_transformer = tf.keras.Sequential([\n",
    "    layers.TextVectorization(max_tokens=vocab_size, output_sequence_length=max_length),\n",
    "    layers.Transformer(num_heads=2, d_model=32, num_layers=2, dropout=0.2),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile and train the models\n",
    "model_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_rnn.fit(train_data, train_labels, epochs=10)\n",
    "\n",
    "model_transformer.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_transformer.fit(train_data, train_labels, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5b36283-6c0c-4458-a668-0d9141715661",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-151638418117575>, line 22\u001B[0m\n",
       "\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Compile and train the model\u001B[39;00m\n",
       "\u001B[1;32m     21\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbinary_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m, optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
       "\u001B[0;32m---> 22\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(image_data, labels, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
       "\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n",
       "\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n",
       "\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1105\u001B[0m, in \u001B[0;36mselect_data_adapter\u001B[0;34m(x, y)\u001B[0m\n",
       "\u001B[1;32m   1102\u001B[0m adapter_cls \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mcls\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01min\u001B[39;00m ALL_ADAPTER_CLS \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mcan_handle(x, y)]\n",
       "\u001B[1;32m   1103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m adapter_cls:\n",
       "\u001B[1;32m   1104\u001B[0m     \u001B[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001B[39;00m\n",
       "\u001B[0;32m-> 1105\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m   1106\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to find data adapter that can handle input: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n",
       "\u001B[1;32m   1107\u001B[0m             _type_name(x), _type_name(y)\n",
       "\u001B[1;32m   1108\u001B[0m         )\n",
       "\u001B[1;32m   1109\u001B[0m     )\n",
       "\u001B[1;32m   1110\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(adapter_cls) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
       "\u001B[1;32m   1111\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n",
       "\u001B[1;32m   1112\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData adapters should be mutually exclusive for \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1113\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhandling inputs. Found multiple adapters \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m to handle \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1114\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(adapter_cls, _type_name(x), _type_name(y))\n",
       "\u001B[1;32m   1115\u001B[0m     )\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: Failed to find data adapter that can handle input: <class 'tensorflow.python.framework.ops.EagerTensor'>, <class 'ellipsis'>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ValueError",
        "evalue": "Failed to find data adapter that can handle input: <class 'tensorflow.python.framework.ops.EagerTensor'>, <class 'ellipsis'>"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: Failed to find data adapter that can handle input: <class 'tensorflow.python.framework.ops.EagerTensor'>, <class 'ellipsis'>"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
        "File \u001B[0;32m<command-151638418117575>, line 22\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Compile and train the model\u001B[39;00m\n\u001B[1;32m     21\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbinary_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m, optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m---> 22\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(image_data, labels, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1105\u001B[0m, in \u001B[0;36mselect_data_adapter\u001B[0;34m(x, y)\u001B[0m\n\u001B[1;32m   1102\u001B[0m adapter_cls \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mcls\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01min\u001B[39;00m ALL_ADAPTER_CLS \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mcan_handle(x, y)]\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m adapter_cls:\n\u001B[1;32m   1104\u001B[0m     \u001B[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001B[39;00m\n\u001B[0;32m-> 1105\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1106\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to find data adapter that can handle input: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1107\u001B[0m             _type_name(x), _type_name(y)\n\u001B[1;32m   1108\u001B[0m         )\n\u001B[1;32m   1109\u001B[0m     )\n\u001B[1;32m   1110\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(adapter_cls) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   1111\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1112\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData adapters should be mutually exclusive for \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1113\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhandling inputs. Found multiple adapters \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m to handle \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1114\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001B[1;32m   1115\u001B[0m     )\n",
        "\u001B[0;31mValueError\u001B[0m: Failed to find data adapter that can handle input: <class 'tensorflow.python.framework.ops.EagerTensor'>, <class 'ellipsis'>"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "\n",
    "# Load and preprocess the image data\n",
    "image_file_paths = ['medical_image1.dcm', 'medical_image2.dcm', ...]  # List of image file paths\n",
    "image_data = []  # List to store preprocessed image data\n",
    "\n",
    "image_data = tf.stack(image_data)  # Convert image_data list to a TensorFlow tensor\n",
    "\n",
    "# Define a CNN model for image analysis\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    # ... additional layers\n",
    "    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(image_data, labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5854a72f-0ab1-4f25-a9a0-fed8e9bb6175",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the trained model for prediction\n",
    "prediction = model.predict(image_array.reshape(1, 224, 224, 3))\n",
    "\n",
    "# ... post-processing and interpretation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb98ba8a-92f2-408e-a0c8-4374efea8e57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import lime\n",
    "import shap\n",
    "\n",
    "# Explain model predictions using LIME (example)\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanation = explainer.explain_instance(image_array, model.predict)\n",
    "explanation.show_in_notebook()\n",
    "\n",
    "# Use SHAP for feature importance analysis\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adc1a5fb-117c-4485-a1eb-b6eb06762da1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Example augmentation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Apply transformations to images during training\n",
    "augmented_image = transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b369f8b9-9b16-4298-b9ca-cefe79785c8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.001],\n",
    "    'batch_size': [32, 64],\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(image_data, labels)\n",
    "\n",
    "# Best model and parameters\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c65922b9-02dd-4b80-b46c-69aec9c9d050",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create an ensemble of models\n",
    "ensemble_model = RandomForestClassifier(n_estimators=100)\n",
    "ensemble_model.fit(image_data, labels)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Medical Diagnosis AI Code",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
